# RiveAvatarLipsync - Comprehensive Documentation

> Logging note: Use the structured logger in all examples. Assume `const logger = getClientLogger('RiveAvatarLipsyncDocs');` and replace any legacy `console.*` calls with `logger.info|warn|error`.

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Quick Start](#quick-start)
4. [Configuration](#configuration)
5. [Critical Issues Fixed](#critical-issues-fixed)
6. [Technical Implementation](#technical-implementation)
7. [Animation Control System](#animation-control-system)
8. [Speech Detection System](#speech-detection-system)
9. [RULE 6 Enforcement](#rule-6-enforcement)
10. [Intelligence Layer](#intelligence-layer)
11. [Performance & Monitoring](#performance--monitoring)
12. [Development & Debugging](#development--debugging)
13. [VAPI Integration](#vapi-integration)
14. [Security & Safety](#security--safety)
15. [Testing Strategy](#testing-strategy)
16. [Troubleshooting](#troubleshooting)
17. [API Reference](#api-reference)

---

## ğŸ¯ Overview

The RiveAvatarLipsync feature provides sophisticated real-time lip-sync animation control for Rive avatars with advanced voice confusion prevention and intelligent speech detection. This system ensures natural, responsive mouth movements that respect user speech priority while providing robust fallback mechanisms.

### ğŸš€ Key Features

- **ğŸ­ Sophisticated Animation Control**: Multi-stage Rive state machine integration with dynamic intensity calculation
- **ğŸ§  Intelligent Speech Detection**: Transcript-based triggering with multi-factor confidence scoring
- **ğŸš« Voice Confusion Prevention**: Prevents feedback loops from assistant's own speech being detected as input
- **ğŸ‘¤ User Priority Enforcement**: RULE 6 - User speech ALWAYS overrides assistant animations
- **ğŸ”„ Real-time State Management**: Ultra-fast 50ms response time for user speech detection
- **ğŸ“Š Performance Monitoring**: Comprehensive metrics and debugging capabilities
- **ğŸšï¸ Fallback Modes**: Volume-based animation when transcript data is unavailable

### ğŸ¯ Performance Characteristics

- **âš¡ 50ms Response Time**: Ultra-fast user speech detection
- **ğŸ¯ Multi-Factor Confidence**: Dynamic scoring based on content analysis
- **ğŸ”„ Real-Time State Switching**: Seamless transitions between animation modes
- **ğŸ“ˆ Dynamic Intensity**: Content-aware animation intensity calculation
- **ğŸ› Comprehensive Debugging**: Visual indicators and detailed console logging

---

## ğŸ—ï¸ Architecture

The feature follows the canonical feature architecture pattern from the Nia Universal Developer Guide:

```
apps/interface/src/features/RiveAvatarLipsync/
â”œâ”€â”€ definition.ts               # Dynamic content definition
â”œâ”€â”€ types/                      # TypeScript interfaces
â”‚   â””â”€â”€ lipsync-types.ts       
â”œâ”€â”€ lib/                        # Client-side helpers and hooks
â”‚   â”œâ”€â”€ useLipsyncSpeechDetection.ts  # Speech detection & processing
â”‚   â””â”€â”€ useAnimationControl.ts        # Animation state control
â”œâ”€â”€ components/                 # UI components
â”‚   â”œâ”€â”€ RiveAvatarLipsync.tsx          # Main avatar component
â”‚   â””â”€â”€ LipsyncDebugPanel.tsx          # Development debug panel
â”œâ”€â”€ services/                   # External orchestration
â”‚   â””â”€â”€ LipsyncService.ts              # Central service class
â”œâ”€â”€ actions/                    # Server actions
â”‚   â””â”€â”€ lipsync-actions.ts             # External API wrappers
â”œâ”€â”€ __tests__/                  # Comprehensive tests
â”‚   â”œâ”€â”€ animation-control.test.ts
â”‚   â”œâ”€â”€ speech-detection.test.ts
â”‚   â”œâ”€â”€ lipsync-service.test.ts
â”‚   â””â”€â”€ integration.test.tsx
â”œâ”€â”€ examples/                   # Usage examples
â”‚   â””â”€â”€ usage-example.tsx
â””â”€â”€ index.ts                    # Barrel exports
```

### ğŸ”„ Data Flow

```
VAPI Messages â†’ Speech Detection â†’ Confidence Scoring â†’ Animation Control â†’ Rive State Machine
     â†“                                                                            â†‘
User Speech Events â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RULE 6 Enforcement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Import Guidelines

Following the Nia Universal Developer Guide, this feature separates server-side and client-side exports to avoid mixing issues:

**âœ… Server-side imports (safe from barrel):**
```tsx
// These can be imported from the barrel file
import { 
  lipsyncService,           // Service singleton
  type LipsyncConfig,       // TypeScript types
  type VAPIMessage,         // TypeScript types
  definition,               // Feature definition
  initializeLipsync,        // Server actions
  startLipsync             // Server actions
} from '@interface/features/RiveAvatarLipsync';
```

**âœ… Client-side imports (direct path required):**
```tsx
// These must be imported directly to avoid server/client mixing
import { RiveAvatarLipsync } from '@interface/features/RiveAvatarLipsync/components/RiveAvatarLipsync';
import { LipsyncDebugPanel } from '@interface/features/RiveAvatarLipsync/components/LipsyncDebugPanel';
import { useAnimationControl } from '@interface/features/RiveAvatarLipsync/lib/useAnimationControl';
import { useLipsyncSpeechDetection } from '@interface/features/RiveAvatarLipsync/lib/useLipsyncSpeechDetection';
```

### Basic Usage

```tsx
// Import client components directly to avoid server/client mixing
import { RiveAvatarLipsync } from '@interface/features/RiveAvatarLipsync/components/RiveAvatarLipsync';

function MyComponent() {
  return (
    <RiveAvatarLipsync 
      className="my-avatar"
      width={300}
      height={300}
      enableDebug={process.env.NODE_ENV === 'development'}
    />
  );
}
```

### Advanced Hook Usage

```tsx
// Import hooks directly to avoid server/client mixing
import { useAnimationControl } from '@interface/features/RiveAvatarLipsync/lib/useAnimationControl';
import { getClientLogger } from '@interface/lib/client-logger';

function CustomAvatarController() {
  const { 
    animationState, 
    speechState, 
    forceStopAnimations, 
    resumeAnimations 
  } = useAnimationControl();

  // Access real-time animation state with structured logger
  const log = getClientLogger('RiveAvatarLipsyncDocs');
  log.info('Animation state snapshot', {
    animationType: animationState.animationType,
    shouldAnimate: animationState.shouldShowTalkingAnimation,
    speechConfidence: speechState.assistantSpeechConfidence
  });

  return (
    <div>
      <div>Status: {animationState.animationType}</div>
      <button onClick={forceStopAnimations}>Force Stop</button>
      <button onClick={resumeAnimations}>Resume</button>
    </div>
  );
}
```

### Service Integration

```tsx
// Import server-side services from barrel (this is safe)
import { lipsyncService } from '@interface/features/RiveAvatarLipsync';

// Initialize with custom configuration
await lipsyncService.initialize({
  enabled: true,
  useRiveAnimations: true,
  voiceConfusion: {
    transcriptOnlyTriggers: true,
    userSpeechResponseTime: 50
  }
});

// Start processing
lipsyncService.start();

// Process VAPI messages
lipsyncService.processVAPIMessage({
  type: 'transcript',
  role: 'assistant',
  transcript: 'Hello, how can I help you?',
  transcriptType: 'final'
});
```

---

## ğŸ”§ Configuration

### LipsyncConfig Interface

```typescript
interface LipsyncConfig {
  enabled: boolean;                    // Enable/disable feature
  useRiveAnimations: boolean;          // Rive vs CSS animations
  
  riveConfig: {
    src: string;                      // Path to .riv file
    stateMachineName: string;         // State machine name
    stages: {                         // Animation stages
      STARTING: number;
      RELAXED_SPEAKING: number;
      BROWSER_EXPLANATION: number;
      CALL_ENDING: number;
    };
    relaxedStageValues: {
      IDLE: number;
      SMILE_BASIC: number;
      RELAX_TALK: number;
      TALKING: number;
    };
    browserStageValues: {
      IDLE: number;
      RELAX_TALK: number;
      LOOKS_LEFT: number;
      TALKS_WHILE_LOOKING_LEFT: number;
    };
  };
  
  voiceConfusion: {
    transcriptOnlyTriggers: boolean;  // Use only transcript data
    userSpeechResponseTime: number;   // Response time (ms)
    speechEndTimeout: number;         // Cleanup delay (ms)
  };
  
  confidenceWeights: {
    contentLength: number;            // Weight for content length
    transcriptFinality: number;       // Weight for final transcripts
    substantialContent: number;       // Weight for substantial content
    recentActivity: number;           // Weight for recent activity
  };
  
  debug: {
    enableLogging: boolean;           // Console logging
    showDebugPanel: boolean;          // Show debug UI
    logStateChanges: boolean;         // Log state changes
  };
}
```

### Default Configuration

```typescript
const DEFAULT_CONFIG = {
  enabled: true,
  useRiveAnimations: true,
  voiceConfusion: {
    transcriptOnlyTriggers: true,
    userSpeechResponseTime: 50,      // 50ms ultra-fast response
    speechEndTimeout: 1500
  },
  confidenceWeights: {
    contentLength: 0.4,              // 40% weight
    transcriptFinality: 0.3,         // 30% weight
    substantialContent: 0.2,         // 20% weight
    recentActivity: 0.1              // 10% weight
  },
  debug: {
    enableLogging: process.env.NODE_ENV === 'development',
    showDebugPanel: false,
    logStateChanges: true
  }
};
```

---

## ğŸš¨ Critical Issues Fixed

### Issue 1: Assistant Mouth Moving During User Speech (RULE 6 Violations)

**Problem**: Assistant's mouth continued animating when the user was speaking, creating an unnatural conversation experience.

**Root Causes**:
1. **Ambiguous Event Handling**: Vapi's `speech-start` and `speech-end` events weren't properly differentiated between user and assistant speech
2. **Missing User Priority**: Animation control logic didn't adequately enforce user speech priority
3. **Volume-Based Confusion**: Volume detection incorrectly triggered assistant animations during user speech
4. **Insufficient Safety Checks**: Multiple code paths could allow animations even when user was speaking

**Solution Implemented**:

#### Enhanced Speech Context Detection (`speech-context.tsx`)

**Before**:
```typescript
const onSpeechStart = () => {
  setIsUserSpeaking(true); // Too simplistic
};
```

**After**:
```typescript
const onSpeechStart = () => {
  // Vapi's speech-start primarily indicates USER speech start
  setIsUserSpeaking(true);
  if (ADV_SPEECH) {
    setCanAssistantAnimate(false); // Immediately block assistant animations
  }
  if (process.env.NODE_ENV === 'development') {
    logger.info('ğŸ¤ SPEECH CONTEXT: USER SPEECH START DETECTED', {
      timestamp: new Date().toISOString(),
      wasAssistantSpeaking: isAssistantSpeaking,
      action: 'Immediately blocking assistant animations'
    });
  }
};
```

#### RULE 6 Enforcement (`useLipsyncSpeechDetection.ts`)

```typescript
const handleUserSpeechStart = useCallback(() => {
  if (DEBUG_LOGGING) {
    logger.info('ğŸš« RULE 6 TRIGGERED: USER SPEECH START - IMMEDIATE ANIMATION FREEZE');
  }

  setSpeechState(prev => ({
    ...prev,
    isUserSpeaking: true,
    canAssistantAnimate: false, // IMMEDIATELY block all animations
    assistantVolumeLevel: 0, // Cut assistant volume display
    isAssistantSpeaking: false, // Force stop assistant speech detection
    assistantSpeechConfidence: 0 // Reset confidence
  }));

  // Clear any pending timeouts - user priority is absolute
  if (speechEndTimeoutRef.current) {
    clearTimeout(speechEndTimeoutRef.current);
  }
  if (confidenceDecayTimeoutRef.current) {
    clearTimeout(confidenceDecayTimeoutRef.current);
  }
  
  // Reset last transcript time to prevent volume-based detection override
  lastTranscriptTimeRef.current = 0;
}, []);
```

### Issue 2: Mouth Animation When Assistant Not Speaking

**Problem**: The avatar's mouth was animating even when the assistant was completely silent.

**Solution**: Enhanced animation conditions to require assistant to be actively speaking OR generating text:

**Before**:
```typescript
const shouldAnimate = speechState.isAssistantGeneratingText &&        // Assistant is generating content
                     speechState.isAssistantSpeaking &&             // Assistant is actively speaking  
                     !speechState.isUserSpeaking &&                 // User is NOT speaking
                     speechState.canAssistantAnimate;               // Animations are permitted
```

**After**:
```typescript
// ENHANCED: Stricter conditions - assistant MUST be speaking or generating
const shouldAnimate = (speechState.isAssistantGeneratingText || speechState.isAssistantSpeaking) &&  // Assistant is active
                     !speechState.isUserSpeaking &&                                                  // User is NOT speaking
                     speechState.canAssistantAnimate &&                                              // Animations are permitted
                     hasContent &&                                                                    // There's actual transcript content
                     (isHighConfidence || isQualityTranscript) &&                                     // High confidence OR quality transcript
                     hasRecentActivity;                                                               // Recent speech activity
```

### Issue 3: Insufficient Debugging Capabilities

**Problem**: Limited visibility into animation decision-making process.

**Solution**: Comprehensive logging system with categorized logs:

#### Animation Control Logs
- `ğŸ­ ANIMATION DECISION CYCLE:` - Every animation decision with all inputs
- `âœ… MOUTH ANIMATION APPROVED:` - When animation is permitted
- `âŒ MOUTH ANIMATION DENIED:` - When animation is blocked (with reasons)
- `ğŸ”„ ANIMATION STATE CHANGED:` - State transitions with before/after
- `â–¶ï¸ MOUTH ANIMATION STARTED` - Animation starts
- `â¸ï¸ MOUTH ANIMATION STOPPED` - Animation stops

#### Speech Context Logs
- `ğŸ¤ SPEECH CONTEXT: USER SPEECH START DETECTED` - User begins speaking
- `ğŸ”‡ SPEECH CONTEXT: USER SPEECH END DETECTED` - User stops speaking
- `ğŸ”Š SPEECH CONTEXT: ASSISTANT SPEECH START` - Assistant begins (volume-based)
- `ğŸ—£ï¸ SPEECH CONTEXT: ASSISTANT SPEECH-UPDATE START` - Assistant begins (message-based)
- `ğŸ“¢ SPEECH CONTEXT: SPEECH-UPDATE MESSAGE` - All speech-update messages
- `ğŸ“ SPEECH CONTEXT: ASSISTANT MESSAGE PART` - Transcript generation
- `â° SPEECH CONTEXT: ASSISTANT MESSAGE TIMEOUT` - Message generation timeout
- `ğŸ”‡ SPEECH CONTEXT: ASSISTANT AUTO-STOPPED` - Auto-stop due to silence

#### Safety Logs
- `ğŸš« RULE 6 ENFORCED:` - User speech priority activated
- `ğŸš¨ CRITICAL ERROR:` - RULE 6 violation prevented

---

## ğŸ”§ Technical Implementation

### Design Decisions

#### 1. Architecture Pattern Choice
**Decision**: Follow the canonical feature architecture from the Nia Universal Developer Guide
**Rationale**: 
- Ensures consistency with the existing codebase
- Provides clear separation of concerns
- Enables proper testing and maintainability
- Follows the feature-first development pattern

#### 2. Voice Confusion Prevention Strategy
**Problem**: Assistant's own audio output being detected as speech input, causing feedback loops

**Solution** - Transcript-Based Detection:
```typescript
// âœ… Fixed - Definitive source identification
if (message.type === 'transcript' && message.role === 'assistant') {
  setIsAssistantGeneratingText(true); // Definitely assistant content
  setCanAssistantAnimate(!isUserSpeaking); // Respect user priority
}
```

**Trade-offs**:
- âœ… Eliminates voice confusion completely
- âœ… More accurate speech detection
- âŒ Slight dependency on transcript availability
- âŒ Minimal delay compared to raw audio detection

#### 3. Multi-Layer Safety System for RULE 6
**RULE 6**: User speech ALWAYS overrides assistant animations

**Implementation**: Multiple redundant safety checks:

```typescript
// Layer 1: Condition checking
const shouldAnimate = isAssistantGeneratingText && 
                     !isUserSpeaking &&
                     canAssistantAnimate;

// Layer 2: State validation
if (userSpeaking && newState.shouldShowTalkingAnimation) {
  logger.error('ğŸš¨ RULE 6 VIOLATION: Forcing stop');
  newState.shouldShowTalkingAnimation = false;
  newState.forceStopAnimation = true;
}

// Layer 3: Emergency override
const forceStopAnimations = () => {
  setAnimationState(prev => ({
    ...prev,
    shouldShowTalkingAnimation: false,
    forceStopAnimation: true,
    isUserDominant: true
  }));
};
```

### Implementation Challenges

#### 1. React Hook Dependencies
**Challenge**: Complex dependency arrays causing unnecessary re-renders

**Solution**: Granular dependencies
```typescript
// âœ… Only re-render on specific changes
useEffect(() => {
  updateAnimation();
}, [
  speechState.isUserSpeaking,
  speechState.isAssistantSpeaking,
  speechState.assistantSpeechConfidence,
  speechState.canAssistantAnimate
]);
```

#### 2. Timing and Race Conditions
**Challenge**: User speech events arriving out of order or with timing issues

**Solution**: Timestamp-based state resolution
```typescript
const handleUserSpeechStart = useCallback(() => {
  // Clear any pending timeouts
  if (speechEndTimeoutRef.current) {
    clearTimeout(speechEndTimeoutRef.current);
  }
  
  // Immediately update state
  setSpeechState(prev => ({
    ...prev,
    isUserSpeaking: true,
    canAssistantAnimate: false
  }));
}, []);
```

#### 3. Memory Leaks and Cleanup
**Solutions Implemented**:
1. **Timeout Cleanup**: All timeouts properly cleared on unmount
2. **State Reset**: Service stop() method resets all state
3. **Confidence History Limits**: Keep only last 100 confidence scores
4. **Event Listener Cleanup**: Proper cleanup in useEffect return functions

---

## ğŸ­ Animation Control System

### Animation States

| State | Description | Trigger Conditions |
|-------|-------------|-------------------|
| `idle` | Default quiet state | No speech activity |
| `talking` | Active mouth animation | High-confidence transcript + no user speech |
| `listening` | Attentive but quiet | User speaking or low confidence |
| `frozen` | Emergency stop | User speech override (RULE 6) |
| `volume_based` | Fallback mode | Processing without transcript |

### Animation Intensity Calculation

```typescript
const calculateAnimationIntensity = useCallback(() => {
  let intensity = 0;
  
  // Base volume level (40% weight)
  const baseVolume = Math.min(speechState.assistantVolumeLevel / 100, 0.6);
  intensity += baseVolume * 0.4;
  
  // Confidence boost (30% weight)
  const confidenceBoost = speechState.assistantSpeechConfidence * 0.3;
  intensity += confidenceBoost;
  
  // Content length boost (20% weight)
  const lengthFactor = Math.min(speechState.lastAssistantMessage.length / 50, 1);
  const lengthBoost = lengthFactor * 0.2;
  intensity += lengthBoost;
  
  // Quality boost (10% weight)
  const qualityBoost = speechState.transcriptQuality === 'final' ? 0.1 : 
                      speechState.transcriptQuality === 'partial' ? 0.05 : 0;
  intensity += qualityBoost;
  
  return Math.min(Math.max(intensity, 0.4), 1.0); // 0.4-1.0 range
}, [speechState]);
```

### Rive State Machine Integration

```typescript
const updateAvatarState = useCallback(() => {
  if (!stageInput || !relaxStageInput || !lookLeftInput) return;

  let targetStage = RIVE_CONFIG.stages.RELAXED_SPEAKING;
  let targetRelaxValue = RIVE_CONFIG.relaxedStageValues.IDLE;
  let targetLookLeftValue = RIVE_CONFIG.browserStageValues.IDLE;

  // Stage 1: Relaxed speaking mode (default)
  if (!isBrowserWindowVisible) {
    targetStage = RIVE_CONFIG.stages.RELAXED_SPEAKING;
    
    if (animationState.shouldShowTalkingAnimation && !animationState.forceStopAnimation) {
      const intensity = animationState.intensity;
      if (intensity > 0.8) {
        targetRelaxValue = RIVE_CONFIG.relaxedStageValues.TALKING;
      } else if (intensity > 0.6) {
        targetRelaxValue = RIVE_CONFIG.relaxedStageValues.RELAX_TALK;
      } else {
        targetRelaxValue = RIVE_CONFIG.relaxedStageValues.SMILE_BASIC;
      }
    }
  }
  // Stage 2: Browser explanation mode
  else if (isBrowserWindowVisible) {
    targetStage = RIVE_CONFIG.stages.BROWSER_EXPLANATION;
    
    if (animationState.shouldShowTalkingAnimation && !animationState.forceStopAnimation) {
      targetLookLeftValue = RIVE_CONFIG.browserStageValues.TALKS_WHILE_LOOKING_LEFT;
    } else {
      targetLookLeftValue = RIVE_CONFIG.browserStageValues.LOOKS_LEFT;
    }
  }

  // CRITICAL: If user is speaking, immediately freeze all mouth animations
  if (animationState.forceStopAnimation || animationState.isUserDominant) {
    if (targetStage === RIVE_CONFIG.stages.RELAXED_SPEAKING) {
      targetRelaxValue = RIVE_CONFIG.relaxedStageValues.IDLE;
    } else if (targetStage === RIVE_CONFIG.stages.BROWSER_EXPLANATION) {
      targetLookLeftValue = RIVE_CONFIG.browserStageValues.LOOKS_LEFT;
    }
  }

  // Update state machine inputs with error checking
  try {
    stageInput.value = targetStage;
    relaxStageInput.value = targetRelaxValue;
    lookLeftInput.value = targetLookLeftValue;
  } catch (error) {
    logger.error('Error updating Rive state machine:', error);
  }
}, [animationState, isBrowserWindowVisible, stageInput, relaxStageInput, lookLeftInput]);
```

---

## ğŸ¤ Speech Detection System

### Signal Reliability Hierarchy

1. **Most Reliable**: `transcript` messages with `role: 'assistant'`
2. **Very Reliable**: `conversation-update` with `transcriptType: 'final'`
3. **Reliable**: `model-output` messages
4. **Moderately Reliable**: `assistant-speech-start/end` (if available)
5. **Less Reliable**: Volume-based detection
6. **Avoid**: `speech-update` events (prone to feedback loops)

### Enhanced Speech Detection Logic

```typescript
const processVAPIMessage = useCallback((message: VAPIMessage) => {
  const currentTime = Date.now();
  
  // MOST RELIABLE: Definitive assistant transcript
  if (message.type === 'transcript' && message.role === 'assistant') {
    const confidence = calculateConfidence(
      message.transcript,
      message.transcriptType || 'partial',
      currentTime
    );
    
    const canAnimate = determineAnimationPermission(confidence, currentTime);
    
    setSpeechState(prev => ({
      ...prev,
      isAssistantGeneratingText: true,
      isAssistantSpeaking: true,
      lastAssistantMessage: message.transcript,
      assistantSpeechConfidence: confidence,
      transcriptQuality: message.transcriptType === 'final' ? 'final' : 'partial',
      speechTimestamp: currentTime,
      canAssistantAnimate: canAnimate && !prev.isUserSpeaking
    }));
    
    lastTranscriptTimeRef.current = currentTime;
  }
  
  // VERY RELIABLE: Conversation lifecycle events
  else if (message.type === 'conversation-update') {
    if (message.transcriptType === 'final') {
      setSpeechState(prev => ({
        ...prev,
        transcriptQuality: 'final',
        assistantSpeechConfidence: Math.min(prev.assistantSpeechConfidence + 0.2, 1)
      }));
    }
    
    // Cleanup after conversation end
    const cleanupDelay = 2000;
    setTimeout(() => {
      setSpeechState(prev => ({
        ...prev,
        isAssistantGeneratingText: false,
        isAssistantSpeaking: false,
        canAssistantAnimate: false,
        assistantSpeechConfidence: 0,
        lastAssistantMessage: '',
        transcriptQuality: 'none'
      }));
    }, cleanupDelay);
  }
  
  // LESS RELIABLE: Volume-based detection (use with caution)
  else if (message.type === 'speech-update') {
    const speaking = message.status === 'started';
    
    // CRITICAL: Differentiate between user and assistant speech-update events
    if (message.role === 'user') {
      setSpeechState(prev => ({
        ...prev,
        isUserSpeaking: speaking,
        canAssistantAnimate: speaking ? false : prev.canAssistantAnimate
      }));
    } else if (message.role === 'assistant' || !message.role) {
      // Don't override transcript-based detection if we have recent transcript data
      if (currentTime - lastTranscriptTimeRef.current > 2000) {
        setSpeechState(prev => ({
          ...prev,
          isAssistantSpeaking: speaking,
          assistantVolumeLevel: speaking ? prev.assistantVolumeLevel : 0
        }));
      }
    }
  }
}, [calculateConfidence, determineAnimationPermission]);
```

---

## ğŸš« RULE 6 Enforcement

**The Golden Rule**: User speech ALWAYS overrides assistant animations, regardless of any other conditions.

### Multi-Layer Protection System

#### Layer 1: Primary Detection
```typescript
const onSpeechStart = () => {
  setIsUserSpeaking(true);
  if (ADV_SPEECH) {
    setCanAssistantAnimate(false); // Immediately block assistant animations
  }
};
```

#### Layer 2: Hook-Level Enforcement
```typescript
const handleUserSpeechStart = useCallback(() => {
  setSpeechState(prev => ({
    ...prev,
    isUserSpeaking: true,
    canAssistantAnimate: false, // IMMEDIATELY block all animations
    assistantVolumeLevel: 0, // Cut assistant volume display
    isAssistantSpeaking: false, // Force stop assistant speech detection
    assistantSpeechConfidence: 0 // Reset confidence
  }));
  
  // Clear any pending timeouts - user priority is absolute
  if (speechEndTimeoutRef.current) {
    clearTimeout(speechEndTimeoutRef.current);
  }
}, []);
```

#### Layer 3: Animation Control Validation
```typescript
if (userSpeaking) {
  // ğŸš« ABSOLUTE PRIORITY: User is speaking - FREEZE everything immediately (RULE 6)
  newState = {
    shouldShowTalkingAnimation: false,
    forceStopAnimation: true,
    animationType: 'frozen',
    intensity: 0,
    isUserDominant: true,
    animationName: 'Avatar Transition'
  };
}
```

#### Layer 4: Emergency Safety Check
```typescript
// CRITICAL: Multiple safety checks to ensure RULE 6 is never violated
if (userSpeaking && newState.shouldShowTalkingAnimation) {
  logger.error('ğŸš¨ CRITICAL ERROR: About to violate RULE 6! Forcing stop.');
  newState.shouldShowTalkingAnimation = false;
  newState.forceStopAnimation = true;
  newState.isUserDominant = true;
}
```

### RULE 6 Violation Monitoring

```typescript
// Monitor for RULE 6 violations in debug panel
useEffect(() => {
  if (speechState.isUserSpeaking && animationState.shouldShowTalkingAnimation) {
    setViolationCount(prev => prev + 1);
    setLastViolation(new Date().toLocaleTimeString());
    logger.error('ğŸš¨ RULE 6 VIOLATION DETECTED: User speaking but animation active!');
  }
}, [speechState.isUserSpeaking, animationState.shouldShowTalkingAnimation]);
```

---

## ğŸ§  Intelligence Layer

### Confidence Scoring Algorithm

```typescript
const calculateConfidence = useCallback((
  transcriptText: string,
  transcriptType: 'partial' | 'final',
  timestamp: number
): number => {
  let confidence = 0;
  
  // Content Length Factor (40% weight)
  if (transcriptText.length > 0) confidence += 0.4;
  
  // Transcript Finality Factor (30% weight)
  if (transcriptType === 'final') confidence += 0.3;
  else if (transcriptType === 'partial') confidence += 0.15;
  
  // Substantial Content Factor (20% weight)
  if (transcriptText.length > 10) confidence += 0.2;
  
  // Recent Activity Factor (10% weight)
  const timeSinceActivity = Date.now() - timestamp;
  if (timeSinceActivity < 10000) { // 10 seconds
    confidence += 0.1 * Math.max(0, 1 - (timeSinceActivity / 10000));
  }
  
  return Math.min(confidence, 1.0);
}, []);
```

### Animation Permission Logic

```typescript
const determineAnimationPermission = useCallback((
  confidence: number,
  timestamp: number
): boolean => {
  const hasMinimumConfidence = confidence > 0.5;
  const hasRecentActivity = Date.now() - timestamp < 10000;
  const hasSubstantialContent = speechState.lastAssistantMessage.length > 5;
  
  return hasMinimumConfidence && hasRecentActivity && hasSubstantialContent;
}, [speechState.lastAssistantMessage]);
```

### Enhanced Mouth Animation Decision Logic

```typescript
const shouldAnimateMouth = useCallback(() => {
  const hasContent = speechState.lastAssistantMessage.length > 0;
  const isHighConfidence = speechState.assistantSpeechConfidence > 0.5;
  const hasRecentActivity = Date.now() - speechState.speechTimestamp < 10000;
  const isQualityTranscript = speechState.transcriptQuality === 'final' || 
                              speechState.transcriptQuality === 'partial';
  
  // ENHANCED: Stricter conditions - assistant MUST be speaking or generating
  const shouldAnimate = (speechState.isAssistantGeneratingText || speechState.isAssistantSpeaking) &&  // Assistant is active
                       !speechState.isUserSpeaking &&                                                  // User is NOT speaking
                       speechState.canAssistantAnimate &&                                              // Animations are permitted
                       hasContent &&                                                                    // There's actual transcript content
                       (isHighConfidence || isQualityTranscript) &&                                     // High confidence OR quality transcript
                       hasRecentActivity;                                                               // Recent speech activity

  return shouldAnimate;
}, [speechState]);
```

---

## ğŸ“Š Performance & Monitoring

### Key Performance Indicators

1. **Response Time**: Time from user speech start to animation stop (Target: <50ms)
2. **Accuracy Rate**: Percentage of correct animation decisions
3. **RULE 6 Violations**: Number of user priority violations (Target: 0)
4. **Confidence Distribution**: Histogram of confidence scores
5. **Processing Time**: Average time to process VAPI messages

### Metrics Collection

```typescript
export class LipsyncService {
  private metrics = {
    messagesProcessed: 0,
    animationStateChanges: 0,
    rule6Violations: 0,
    confidenceScores: [] as number[],
    lastProcessingTime: 0,
    averageConfidence: 0,
    isRunning: false
  };
  
  processVAPIMessage(message: VAPIMessage) {
    const startTime = Date.now();
    
    // ... processing logic
    
    this.metrics.lastProcessingTime = Date.now() - startTime;
    this.metrics.messagesProcessed++;
    
    // Update running averages
    this.updateMetrics();
  }
  
  getMetrics() {
    return { ...this.metrics };
  }
}
```

### Performance Optimizations

#### 1. Debouncing and Throttling
```typescript
// Debounce rapid speech state changes
const debouncedUpdateState = useCallback(
  debounce((newState) => {
    setSpeechState(newState);
  }, 50), // 50ms debounce
  []
);
```

#### 2. Memoized Calculations
```typescript
// Cache expensive calculations
const memoizedIntensity = useMemo(() => {
  return calculateAnimationIntensity();
}, [
  speechState.assistantVolumeLevel,
  speechState.assistantSpeechConfidence,
  speechState.lastAssistantMessage.length
]);
```

#### 3. State Management Optimization
```typescript
// âœ… Efficient immutable updates
setAnimationState(prev => ({
  ...prev,
  intensity: newIntensity,
  shouldShowTalkingAnimation: shouldAnimate
}));
```

---

## ğŸ› ï¸ Development & Debugging

### Debug Panel

Enable the debug panel for real-time monitoring:

```tsx
<RiveAvatarLipsync enableDebug={true} />
```

The debug panel shows:
- âœ… Animation state and intensity
- ğŸ¤ Speech detection status
- ğŸ§  Confidence scoring
- ğŸš¨ RULE 6 violation tracking
- ğŸ“Š Performance metrics

### Comprehensive Logging System

Set `NODE_ENV=development` to enable comprehensive console logging.

#### Log Categories

**ğŸ­ Animation Control Logs**:
- `ğŸ­ ANIMATION DECISION CYCLE:` - Every animation decision with all inputs
- `âœ… MOUTH ANIMATION APPROVED:` - When animation is permitted
- `âŒ MOUTH ANIMATION DENIED:` - When animation is blocked (with reasons)
- `ğŸ”„ ANIMATION STATE CHANGED:` - State transitions with before/after
- `â–¶ï¸ MOUTH ANIMATION STARTED` - Animation starts
- `â¸ï¸ MOUTH ANIMATION STOPPED` - Animation stops

**ğŸ¤ Speech Context Logs**:
- `ğŸ¤ SPEECH CONTEXT: USER SPEECH START DETECTED` - User begins speaking
- `ğŸ”‡ SPEECH CONTEXT: USER SPEECH END DETECTED` - User stops speaking
- `ğŸ”Š SPEECH CONTEXT: ASSISTANT SPEECH START` - Assistant begins (volume-based)
- `ğŸ—£ï¸ SPEECH CONTEXT: ASSISTANT SPEECH-UPDATE START` - Assistant begins (message-based)
- `ğŸ“¢ SPEECH CONTEXT: SPEECH-UPDATE MESSAGE` - All speech-update messages
- `ğŸ“ SPEECH CONTEXT: ASSISTANT MESSAGE PART` - Transcript generation
- `â° SPEECH CONTEXT: ASSISTANT MESSAGE TIMEOUT` - Message generation timeout
- `ğŸ”‡ SPEECH CONTEXT: ASSISTANT AUTO-STOPPED` - Auto-stop due to silence

**ğŸš« Safety Logs**:
- `ğŸš« RULE 6 ENFORCED:` - User speech priority activated
- `ğŸš¨ CRITICAL ERROR:` - RULE 6 violation prevented

#### Log Structure

Each log includes:
- **Timestamp**: ISO format for precise timing
- **Context**: Current state of all relevant variables
- **Action**: What the system is doing as a result
- **Reason**: Why the decision was made (for denials)

#### Debugging Workflow

1. **Open Browser Console** in development mode
2. **Filter by emojis** to focus on specific areas:
   - Filter `ğŸ­` for animation decisions
   - Filter `ğŸ¤` for speech detection
   - Filter `ğŸš«` for RULE 6 enforcement
3. **Watch for patterns**:
   - Animations starting when they shouldn't
   - Missing user speech detection
   - Incorrect state transitions

### Testing

Run the comprehensive test suite:

```bash
npm test RiveAvatarLipsync
```

Tests cover:
- âœ… Animation control logic
- âœ… Speech detection algorithms
- âœ… RULE 6 violation prevention
- âœ… Confidence scoring accuracy
- âœ… Service lifecycle management
- âœ… Error handling

---

## ğŸ”— VAPI Integration

### Message Processing

The system processes VAPI messages through a sophisticated pipeline:

```typescript
// High-priority transcript messages
{
  type: 'transcript',
  role: 'assistant',
  transcript: 'Hello, how can I help you today?',
  transcriptType: 'final'
}

// Conversation lifecycle events
{
  type: 'conversation-update',
  role: 'assistant',
  transcriptType: 'final'
}

// Fallback volume detection (use with caution)
{
  type: 'speech-update',
  status: 'started'
}
```

### Voice Confusion Prevention

The system prevents feedback loops where the assistant's own speech through speakers gets picked up by the microphone:

1. **Transcript-Only Triggers**: Never rely on ambiguous audio signals
2. **Source Identification**: Distinguish between user and assistant speech definitively
3. **Multi-Layer Safety**: Multiple redundant systems prevent violations
4. **Enhanced Debug Logging**: Crystal clear debug information for troubleshooting

### Recommended Vapi Configuration

Based on research, implement these Vapi settings for optimal interruption handling:

```javascript
{
  "stopSpeakingPlan": {
    "numWords": 0,        // Use VAD instead of word count
    "voiceSeconds": 0.2,  // 200ms for fast interruption detection
    "backoffSeconds": 1.0 // 1 second pause before assistant resumes
  }
}
```

---

## ğŸ”’ Security & Safety

### Input Validation

All VAPI messages are treated as untrusted input:

```typescript
const processVAPIMessage = (message: VAPIMessage) => {
  // Validate message structure
  if (!message || typeof message.type !== 'string') {
    logger.warn('Invalid VAPI message:', message);
    return;
  }
  
  // Sanitize transcript content
  const transcript = message.transcript?.slice(0, 1000) || ''; // Limit length
  
  // ... process safely
};
```

### State Protection

Immutable state updates prevent accidental state corruption:

```typescript
// âœ… Safe state updates
setSpeechState(prev => ({
  ...prev,
  lastAssistantMessage: sanitizeMessage(transcript)
}));
```

### Error Boundaries

Component-level error boundaries prevent crashes:

```tsx
<ErrorBoundary fallback={<div>Lipsync unavailable</div>}>
  <RiveAvatarLipsync />
</ErrorBoundary>
```

### Safety Guarantees

The implemented solution provides multiple layers of protection:

1. **Primary**: Enhanced Vapi event handling with immediate user detection
2. **Secondary**: Animation control logic with user permission checks
3. **Tertiary**: Emergency safety checks before any animation starts
4. **Monitoring**: Real-time violation detection and alerting

---

## ğŸ§ª Testing Strategy

### Test Architecture

**Layered Testing Approach**:
1. **Unit Tests**: Pure functions and individual hooks
2. **Integration Tests**: Hook interactions and service integration
3. **Component Tests**: React component rendering and behavior
4. **System Tests**: End-to-end feature functionality

### Mock Strategy

**Speech Context Mocking**:
```typescript
const mockSpeechContext = {
  isAssistantSpeaking: false,
  isUserSpeaking: false,
  canAssistantAnimate: false,
  assistantSpeechConfidence: 0,
  // ... other properties
};

jest.mock('@interface/contexts/speech-context', () => ({
  useSpeech: () => mockSpeechContext
}));
```

### RULE 6 Violation Testing

**Critical Test Case**:
```typescript
it('should never violate RULE 6', () => {
  // Simulate violation attempt
  act(() => {
    Object.assign(mockSpeechState, {
      isUserSpeaking: true,
      isAssistantSpeaking: true,
      canAssistantAnimate: true, // This would be incorrect
    });
  });

  // Should never animate when user is speaking
  expect(result.current.animationState.shouldShowTalkingAnimation).toBe(false);
  expect(result.current.animationState.forceStopAnimation).toBe(true);
});
```

### Manual Testing Checklist

1. âœ… Start assistant speaking â†’ interrupt with user speech â†’ verify immediate stop
2. âœ… User speaks during silence â†’ verify no assistant animation starts
3. âœ… Multiple rapid interruptions â†’ verify system stability
4. âœ… Background noise â†’ verify no false positive triggers
5. âœ… Assistant resumes after user stops â†’ verify smooth transition

---

## ğŸ”§ Troubleshooting

### Common Issues

**Q: Animations not showing despite assistant speaking**
A: Check the following in console logs:
- `canAssistantAnimate` should be `true`
- Confidence score should be > 0.5
- Look for `âŒ MOUTH ANIMATION DENIED:` logs with reasons
- Verify `isAssistantGeneratingText` or `isAssistantSpeaking` is `true`

**Q: User speech not stopping animations**
A: Verify in console logs:
- Look for `ğŸ¤ SPEECH CONTEXT: USER SPEECH START DETECTED` logs
- Check that `ğŸš« RULE 6 ENFORCED` logs appear
- Ensure no `ğŸš¨ CRITICAL ERROR` logs indicating violations
- Verify VAPI user speech events are being processed

**Q: Choppy or delayed animations**
A: Check performance metrics:
- Look for `lastProcessingTime` > 50ms in debug panel
- Monitor `ğŸ­ ANIMATION DECISION CYCLE` frequency
- Check for excessive re-renders in React DevTools
- Consider reducing confidence calculation complexity

**Q: False positive speech detection**
A: Enable stricter settings:
- Set `transcriptOnlyTriggers: true` to avoid volume-based false positives
- Increase `userSpeechResponseTime` for less sensitivity
- Monitor `ğŸ“¢ SPEECH CONTEXT: SPEECH-UPDATE MESSAGE` logs for unwanted triggers

### Debug Commands

```typescript
// Log current state
logger.info('Animation State:', lipsyncService.getAnimationState());
logger.info('Metrics:', lipsyncService.getMetrics());

// Force reset
lipsyncService.stop();
lipsyncService.start();

// Test confidence calculation
const confidence = calculateConfidence('Test message', 'final', Date.now());
logger.info('Confidence:', confidence);

// Check RULE 6 compliance
if (speechState.isUserSpeaking && animationState.shouldShowTalkingAnimation) {
  logger.error('ğŸš¨ RULE 6 VIOLATION DETECTED!');
}
```

### Performance Monitoring

Monitor key metrics in production:

```typescript
const metrics = await lipsyncService.getMetrics();
logger.info('Messages Processed:', metrics.messagesProcessed);
logger.info('Animation State Changes:', metrics.animationStateChanges);
logger.info('RULE 6 Violations:', metrics.rule6Violations); // Should be 0
logger.info('Average Confidence:', metrics.averageConfidence);
logger.info('Processing Time:', metrics.lastProcessingTime);
```

---

## ğŸ“š API Reference

### Core Components

#### RiveAvatarLipsync
```typescript
interface RiveAvatarLipsyncProps {
  className?: string;
  width?: number;
  height?: number;
  enableDebug?: boolean;
}
```

#### LipsyncDebugPanel
```typescript
interface LipsyncDebugPanelProps {
  className?: string;
  compact?: boolean;
}
```

### Hooks

#### useAnimationControl
```typescript
interface UseAnimationControlReturn {
  animationState: AnimationState;
  speechState: SpeechDetectionState;
  forceStopAnimations: () => void;
  resumeAnimations: () => void;
}
```

#### useLipsyncSpeechDetection
```typescript
interface UseLipsyncSpeechDetectionReturn {
  speechState: SpeechDetectionState;
  processVAPIMessage: (message: VAPIMessage) => void;
  calculateConfidence: (text: string, type: string, timestamp: number) => number;
}
```

### Service API

#### LipsyncService
```typescript
class LipsyncService {
  initialize(config: Partial<LipsyncConfig>): Promise<void>;
  start(): void;
  stop(): void;
  processVAPIMessage(message: VAPIMessage): void;
  updateConfig(updates: Partial<LipsyncConfig>): void;
  getAnimationState(): AnimationState | null;
  getMetrics(): LipsyncMetrics;
}
```

### Type Definitions

#### AnimationState
```typescript
interface AnimationState {
  shouldShowTalkingAnimation: boolean;
  forceStopAnimation: boolean;
  animationType: 'talking' | 'listening' | 'idle' | 'frozen' | 'volume_based';
  intensity: number; // 0-1
  isUserDominant: boolean;
  animationName: string;
}
```

#### SpeechDetectionState
```typescript
interface SpeechDetectionState {
  isAssistantSpeaking: boolean;
  isUserSpeaking: boolean;
  assistantVolumeLevel: number; // 0-100
  canAssistantAnimate: boolean;
  isAssistantGeneratingText: boolean;
  lastAssistantMessage: string;
  assistantSpeechConfidence: number; // 0-1
  transcriptQuality: 'none' | 'partial' | 'final';
  speechTimestamp: number; // ms epoch
  audioLevel: number; // 0-1
}
```

#### VAPIMessage
```typescript
interface VAPIMessage {
  type: 'transcript' | 'conversation-update' | 'speech-update' | 'model-output';
  role?: 'user' | 'assistant';
  transcript?: string;
  transcriptType?: 'partial' | 'final';
  status?: 'started' | 'ended';
}
```

---

## ğŸ¯ Summary

The RiveAvatarLipsync feature provides a comprehensive, robust solution for real-time avatar lip-sync animation with sophisticated voice confusion prevention and strict user priority enforcement. The system has been battle-tested to ensure:

### âœ… **Key Achievements**

1. **RULE 6 Compliance**: User speech ALWAYS overrides assistant animations with <50ms response time
2. **Voice Confusion Prevention**: Transcript-based detection eliminates feedback loops
3. **Intelligent Animation Control**: Multi-factor confidence scoring for natural animations
4. **Comprehensive Debugging**: Complete visibility into decision-making process
5. **Production Ready**: Full error handling, monitoring, and performance optimization

### ğŸ”§ **Technical Excellence**

- **Multi-Layer Safety**: Multiple redundant systems prevent violations
- **Performance Optimized**: Efficient state management and memoization
- **Fully Tested**: Comprehensive unit, integration, and system tests
- **Well Documented**: Complete logging and debugging capabilities
- **Scalable Architecture**: Service-based design for external integration

### ğŸš€ **Production Deployment**

The feature is ready for production deployment with:
- Feature flags for controlled rollout
- Performance monitoring and metrics
- Graceful degradation on errors
- Security-first input validation
- Comprehensive troubleshooting guides

Built with â¤ï¸ following the Nia Universal feature-first development pattern.
