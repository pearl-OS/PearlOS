"""OpenClawSessionProcessor — Voice Integration Phase 1.

Replaces the standard LLM service in the Pipecat pipeline when running in
``OPENCLAW_SESSION`` mode.  Instead of managing tool schemas and executing
tools locally, this processor delegates *everything* to the OpenClaw Gateway
via its ``/v1/chat/completions`` streaming endpoint.  OpenClaw handles tool
execution server-side (pearlos-tool, web search, message, exec, etc.).

Frame contract (same slot as OpenAILLMService):
    IN  → LLMMessagesFrame | OpenAILLMContextFrame | LLMContextFrame | StartInterruptionFrame
    OUT → LLMFullResponseStartFrame, TextFrame …, LLMFullResponseEndFrame
"""

from __future__ import annotations

import asyncio
import json
import os
from typing import Any

import aiohttp
from loguru import logger

from pipecat.frames.frames import (
    Frame,
    LLMFullResponseEndFrame,
    LLMFullResponseStartFrame,
    LLMMessagesFrame,
    StartInterruptionFrame,
    TextFrame,
)
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor

try:
    from pipecat.frames.frames import LLMContextFrame
except ImportError:
    LLMContextFrame = None

try:
    from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContextFrame
except ImportError:
    OpenAILLMContextFrame = None


class OpenClawSessionProcessor(FrameProcessor):
    """Stream chat completions from the OpenClaw Gateway.

    Parameters
    ----------
    system_prompt : str
        The full system prompt (identity + voice rules + workspace context).
    api_url : str | None
        OpenClaw base URL.  Defaults to ``OPENCLAW_API_URL`` env var or
        ``http://localhost:18789/v1``.
    api_key : str | None
        Bearer token.  Defaults to ``OPENCLAW_API_KEY`` env var.
    model : str | None
        Model identifier.  Defaults to ``BOT_SONNET_MODEL`` env var or
        ``anthropic/claude-sonnet-4-20250514``.
    max_tokens : int
        Max tokens per completion.
    timeout : int
        HTTP timeout in seconds.
    """

    def __init__(
        self,
        system_prompt: str,
        *,
        api_url: str | None = None,
        api_key: str | None = None,
        model: str | None = None,
        session_key: str | None = None,
        max_tokens: int = 4096,
        timeout: int = 180,  # Increased for multi-tool agent turns
        **kwargs: Any,
    ) -> None:
        super().__init__(**kwargs)
        self._api_url = (
            api_url
            or os.getenv("OPENCLAW_API_URL", "http://localhost:18789/v1")
        ).rstrip("/")
        self._api_key = api_key or os.getenv("OPENCLAW_API_KEY", "openclaw-local")
        # Use openclaw:main to explicitly route through the full agent system
        # (tools, skills, exec, memory) rather than relying on the fallback default.
        self._model = model or os.getenv("BOT_OPENCLAW_MODEL", "openclaw:main")
        # Voice gets a dedicated session under the main agent — shares workspace,
        # skills, SOUL.md, MEMORY.md with all other Pearl sessions but doesn't
        # block TUI/Discord with serialized requests.
        self._session_key = session_key or os.getenv(
            "OPENCLAW_SESSION_KEY", "agent:main:voice"
        )
        self._max_tokens = max_tokens
        self._timeout = timeout

        # Conversation history maintained for the session lifetime.
        # Capped to prevent unbounded growth (system + last N turns).
        self._max_history = 40  # ~20 user/assistant pairs
        self._messages: list[dict[str, str]] = [
            {"role": "system", "content": system_prompt},
        ]

        # Reusable HTTP session (lazy-initialized to ensure event loop exists).
        self._http_session: aiohttp.ClientSession | None = None

        # Cancellation flag for in-flight streaming requests.
        self._cancel_event: asyncio.Event = asyncio.Event()

        logger.info(
            f"[OpenClawSession] Initialized — model={self._model} "
            f"url={self._api_url} timeout={self._timeout}s"
        )

    # ------------------------------------------------------------------
    # Frame routing
    # ------------------------------------------------------------------

    async def process_frame(
        self, frame: Frame, direction: FrameDirection
    ) -> None:
        await super().process_frame(frame, direction)

        if isinstance(frame, StartInterruptionFrame):
            await self._handle_interruption(frame, direction)
            return

        # Extract messages from the various context frame types that the
        # upstream context-aggregator might emit.
        messages: list[dict[str, str]] | None = None

        if isinstance(frame, LLMMessagesFrame):
            messages = frame.messages
        elif OpenAILLMContextFrame and isinstance(frame, OpenAILLMContextFrame):
            ctx = frame.context
            messages = (
                ctx.get_messages_for_logging()
                if hasattr(ctx, "get_messages_for_logging")
                else ctx.messages
            )
        elif LLMContextFrame and isinstance(frame, LLMContextFrame):
            ctx = frame.context
            messages = (
                ctx.get_messages_for_logging()
                if hasattr(ctx, "get_messages_for_logging")
                else getattr(ctx, "messages", None)
            )

        if messages is not None:
            await self._run_completion(messages)
        else:
            # Pass through frames we don't handle (audio, control, etc.)
            await self.push_frame(frame, direction)

    # ------------------------------------------------------------------
    # Core streaming completion
    # ------------------------------------------------------------------

    async def _run_completion(self, incoming_messages: list[dict[str, str]]) -> None:
        """Send messages to OpenClaw and stream the response as TextFrames."""

        # Signal any in-flight request to stop.
        self._cancel_event.set()
        self._cancel_event = asyncio.Event()

        # Merge incoming messages into our session history.
        # The context aggregator sends the *full* conversation each time,
        # so we sync: keep our system prompt, then adopt everything after.
        if incoming_messages:
            # Find non-system messages from incoming
            non_system = [m for m in incoming_messages if m.get("role") != "system"]
            # Rebuild: our system prompt + all conversation messages
            self._messages = [self._messages[0]] + non_system

        # Ensure at least one user message (Anthropic requirement).
        if all(m.get("role") == "system" for m in self._messages):
            self._messages.append(
                {"role": "user", "content": "[user has joined the conversation]"}
            )

        # Trim history: keep system prompt + last N non-system messages
        non_system = [m for m in self._messages if m.get("role") != "system"]
        if len(non_system) > self._max_history:
            self._messages = [self._messages[0]] + non_system[-self._max_history:]

        cancel = self._cancel_event

        payload = {
            "model": self._model,
            "messages": self._messages,
            "stream": True,
            "max_tokens": self._max_tokens,
            "user": "pearlos-voice",
        }

        await self.push_frame(LLMFullResponseStartFrame())

        full_response_chunks: list[str] = []
        first_token_received = False

        # Background task: emit "still thinking" filler if first token takes too long
        async def _slow_response_filler():
            nonlocal first_token_received
            await asyncio.sleep(8.0)
            if not first_token_received and not cancel.is_set():
                await self.push_frame(TextFrame(text="Hmm, let me think about that..."))
            await asyncio.sleep(15.0)
            if not first_token_received and not cancel.is_set():
                await self.push_frame(TextFrame(text="Still working on it..."))

        filler_task = asyncio.create_task(_slow_response_filler())

        try:
            if self._http_session is None or self._http_session.closed:
                self._http_session = aiohttp.ClientSession()
            session = self._http_session
            async with session.post(
                f"{self._api_url}/chat/completions",
                json=payload,
                headers={
                    "Authorization": f"Bearer {self._api_key}",
                    "Content-Type": "application/json",
                    # Route to main agent session — voice IS the main Pearl,
                    # not a separate session. Shares context with Discord/webchat/TUI.
                    "x-openclaw-session-key": self._session_key,
                },
                timeout=aiohttp.ClientTimeout(total=self._timeout),
            ) as resp:
                if resp.status != 200:
                    error_text = await resp.text()
                    logger.error(
                        f"[OpenClawSession] API error {resp.status}: "
                        f"{error_text[:300]}"
                    )
                    await self.push_frame(
                        TextFrame(
                            text="I'm having trouble thinking right now. "
                            "Let me try again in a moment."
                        )
                    )
                    await self.push_frame(LLMFullResponseEndFrame())
                    return

                async for raw_line in resp.content:
                    if cancel.is_set():
                        logger.info("[OpenClawSession] Cancelled mid-stream (interruption)")
                        break
                    line = raw_line.decode("utf-8", errors="replace").strip()
                    if not line.startswith("data:"):
                        continue
                    data_str = line[len("data:"):].strip()
                    if data_str == "[DONE]":
                        break
                    try:
                        chunk = json.loads(data_str)
                        delta = (
                            chunk.get("choices", [{}])[0]
                            .get("delta", {})
                            .get("content")
                        )
                        if delta:
                            first_token_received = True
                            full_response_chunks.append(delta)
                            await self.push_frame(TextFrame(text=delta))
                    except (ValueError, IndexError, KeyError):
                        continue

        except asyncio.CancelledError:
            logger.info("[OpenClawSession] Request cancelled (interruption)")
            filler_task.cancel()
            raise
        except aiohttp.ClientError as exc:
            logger.error(f"[OpenClawSession] Network error: {exc}")
            await self.push_frame(
                TextFrame(
                    text="I lost my connection for a moment. Could you say that again?"
                )
            )
        except Exception as exc:
            logger.exception(f"[OpenClawSession] Unexpected error: {exc}")
            await self.push_frame(
                TextFrame(text="Something went wrong. Let me try again.")
            )
        finally:
            filler_task.cancel()
            # Record assistant response in history.
            if full_response_chunks:
                self._messages.append(
                    {"role": "assistant", "content": "".join(full_response_chunks)}
                )
            await self.push_frame(LLMFullResponseEndFrame())

    # ------------------------------------------------------------------
    # Cleanup
    # ------------------------------------------------------------------

    async def cleanup(self) -> None:
        """Close the shared HTTP session."""
        if self._http_session and not self._http_session.closed:
            await self._http_session.close()
            self._http_session = None
        await super().cleanup()

    # ------------------------------------------------------------------
    # Interruption handling
    # ------------------------------------------------------------------

    async def _handle_interruption(
        self, frame: Frame, direction: FrameDirection
    ) -> None:
        """Signal in-flight request to stop on user interruption."""
        self._cancel_event.set()
        logger.info("[OpenClawSession] Signalled cancel (user interrupted)")
        await self.push_frame(frame, direction)
